{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxhPljy007jKsepDNXO+MC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Humza134/Langchain/blob/main/01_model_prompts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Langchain:\n",
        "### LangChain is an open-source framework designed to streamline and enhance the process of building applications that use language models (LLMs) like OpenAI's GPT-4, Google Gemini, and similar generative models. It provides a set of tools and abstractions to simplify complex workflows, such as document retrieval, question-answering, and chaining multiple model interactions."
      ],
      "metadata": {
        "id": "QIx0PFW_C92c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Required Packages"
      ],
      "metadata": {
        "id": "v6P2MRI-E1rA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFO9p8snBfBD"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U langchain langchain_google_genai\n",
        "%pip install -U langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/feisty-lambda-420416-5d80219f7d8e.json\""
      ],
      "metadata": {
        "id": "zCgin69yChJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's start with a direct API calls to Gemini."
      ],
      "metadata": {
        "id": "7ucznWYwFHDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "57ezEdQyCjJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.generativeai import GenerativeModel\n",
        "# set the api key\n",
        "genai.configure(api_key = GEMINI_API_KEY)\n",
        "\n",
        "llm_model: GenerativeModel = genai.GenerativeModel('gemini-1.5-flash')"
      ],
      "metadata": {
        "id": "sdxAqj2MFjMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def completion(prompt:str, model=llm_model)->str:\n",
        "    \"\"\"\n",
        "    Generate a completion for the given prompt using the specified language model.\n",
        "\n",
        "    Args:\n",
        "    prompt (str): The input prompt to complete.\n",
        "    model (ll_model): The language model to use.\n",
        "\n",
        "    Returns:\n",
        "    str: The completed text.\n",
        "    \"\"\"\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text"
      ],
      "metadata": {
        "id": "ta50HydNHrvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = completion(\"What is the capital of Pakistan?\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "ZniZpNiqIWJk",
        "outputId": "c860c266-00d1-476c-9fc4-7781a5c1c38f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of Pakistan is **Islamabad**. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now let's try how we can do the same thing with langchain"
      ],
      "metadata": {
        "id": "CWtCjq__Ip5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Model Name: specify the model name to use\n",
        "model_name = \"gemini-1.5-flash\"\n",
        "# TEMPERATURE: Control the randomness of model responses (0 = deterministic, 1 = very random)\n",
        "temperature = 0.0\n",
        "\n",
        "def chat_completion(prompt:str, model_name=model_name, temperature=temperature)->str:\n",
        "    \"\"\"\n",
        "    Generate a completion for the given prompt using the specified language model.\n",
        "\n",
        "    Args:\n",
        "    prompt (str): The input prompt to complete.\n",
        "    model_name (str): The name of the language model to use.\n",
        "    temperature (float):\n",
        "    Control the randomness of model responses (0 = deterministic, 1 = very random).\n",
        "\n",
        "    Returns:\n",
        "    str: The completed text.\n",
        "    \"\"\"\n",
        "    llm: ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(\n",
        "        model=model_name,\n",
        "        temperature=temperature,\n",
        "        api_key=GEMINI_API_KEY\n",
        "        )\n",
        "    response = llm.invoke(prompt)\n",
        "    return response"
      ],
      "metadata": {
        "id": "dDVLo_D_Iy45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = chat_completion(\"What is the capital of Pakistan?\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGioMA0lMtka",
        "outputId": "66f1e481-6d3d-4fa0-fcef-26bdb9332531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='The capital of Pakistan is **Islamabad**. \\n' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]} id='run-9a6fa8ed-e022-49cb-b15a-bc7a95970a87-0' usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17, 'input_token_details': {'cache_read': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zINYFqoPLrj6",
        "outputId": "eb707994-9ee8-4182-c3da-b455e2a63262"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The capital of Pakistan is **Islamabad**. \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a nice AI bot.\"),\n",
        "    HumanMessage(content=\"What is the capital of Pakistan?\"),\n",
        "]\n",
        "result = chat_completion(messages)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSwGCCyCM45A",
        "outputId": "f3fe5132-0433-49be-e073-523ae575d84f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='The capital of Pakistan is **Islamabad**. \\n' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]} id='run-16f245bd-e3e6-4f56-9bac-270a03c1129b-0' usage_metadata={'input_tokens': 15, 'output_tokens': 9, 'total_tokens': 24, 'input_token_details': {'cache_read': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Template\n",
        "**A PromptTemplate** in LangChain is a structured way to define and format prompts for language models, enabling dynamic content insertion based on variables. It allows you to create reusable prompts with placeholders that can be customized for various tasks, such as summarization, question-answering, or instruction-following.\n",
        "\n",
        "## Here are the key features of a PromptTemplate:\n",
        "\n",
        "**1.** **Dynamic Variable Insertion**:\n",
        "\n",
        "You can define placeholders  within a prompt **(like {input_variable})** allowing you to customize and reuse templates with different inputs.\n",
        "\n",
        "**2.Template Standardization**:\n",
        "\n",
        "PromptTemplate ensures a consistent structure across prompts, improving readability and maintainability.\n",
        "\n",
        "**3.Ease of Reuse**:\n",
        "\n",
        "Once created, a template can be reused across multiple chains, tasks, or models, saving time and ensuring prompt consistency.\n",
        "\n",
        "**4.Error Reduction**:\n",
        "\n",
        "By defining placeholders in a structured way, it reduces the chance of hardcoded errors and ensures input values are always inserted correctly.\n",
        "\n",
        "**5.Flexible Prompt Construction**:\n",
        "\n",
        "You can easily build complex prompts by combining several templates or modifying placeholders based on specific tasks or requirements.\n",
        "\n",
        "**6.Supports Complex Formatting**:\n",
        "\n",
        "Advanced templates can be created to support various prompt styles, formatting options, or custom instructions for specific tasks or model behaviors."
      ],
      "metadata": {
        "id": "eWyX7WlhOdyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Define a combined template with system instructions and human message\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=(\n",
        "        \"System: You are an intelligent assistant that provides concise summaries.\\n\"\n",
        "        \"Human: What is a summary for the following question: {question}?\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Use the template by providing the variable\n",
        "formatted_prompt = prompt.format(question=\"What is langchain and what is the purpose of langchain?\")\n",
        "print(formatted_prompt)\n",
        "\n",
        "result = chat_completion(formatted_prompt)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68NqSsBTXJmn",
        "outputId": "ed2d187c-3b5e-458e-c295-57110a946c09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System: You are an intelligent assistant that provides concise summaries.\n",
            "Human: What is a summary for the following question: What is langchain and what is the purpose of langchain??\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='LangChain is a framework that simplifies building applications powered by large language models (LLMs). Its purpose is to connect LLMs with external data sources and tools, enabling them to perform complex tasks like question answering, summarization, and code generation. \\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-2f4dc790-cc64-4df7-bafa-5e607b3819cc-0', usage_metadata={'input_tokens': 38, 'output_tokens': 49, 'total_tokens': 87, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"In an easy way to translate the following senetence '{sentence}' into {target_language}:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"sentence\", \"target_language\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "formatted_prompt = prompt.format(sentence=\"How are You?\", target_language=\"Urdu\")\n",
        "print(formatted_prompt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgJjVWDfcMNZ",
        "outputId": "4af57c2b-426a-4043-d9fe-4a851db11faa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In an easy way to translate the following senetence 'How are You?' into Urdu:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = chat_completion(formatted_prompt)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qa8glYlHdP81",
        "outputId": "9c18c4c3-248d-4c2e-8479-59394851cf1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The easiest way to translate \"How are you?\" into Urdu is:\\n\\n**\"Aap kaise hain?\" (آپ کیسے ہیں؟)** \\n\\nThis is the most common and widely understood phrase. \\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-b17ddd53-1190-4448-bf58-5d3c0650d080-0', usage_metadata={'input_tokens': 20, 'output_tokens': 44, 'total_tokens': 64, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XjCcWYSUdWY1",
        "outputId": "7e261c21-9f96-40a8-96aa-31b5afadcad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The easiest way to translate \"How are you?\" into Urdu is:\\n\\n**\"Aap kaise hain?\" (آپ کیسے ہیں؟)** \\n\\nThis is the most common and widely understood phrase. \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "tempalte = \"\"\" create a recipie of {type} cake describe in points.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"type\"],\n",
        "    template=tempalte,\n",
        ")\n",
        "\n",
        "formatted_prompt = prompt.format(type=\"chocolate\")\n",
        "print(formatted_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "as0Ofhihg7Qp",
        "outputId": "4b7da234-272f-4395-98d2-261d1d61f781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " create a recipie of chocolate cake describe in points.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = chat_completion(formatted_prompt)\n",
        "result.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "2fTigyiCjJtC",
        "outputId": "929f51e3-455b-4362-e2f7-56a0970b5ac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"## Chocolate Cake Recipe:\\n\\n**Ingredients:**\\n\\n* **For the Cake:**\\n    * 2 cups all-purpose flour\\n    * 2 cups granulated sugar\\n    * 3/4 cup unsweetened cocoa powder\\n    * 1 teaspoon baking soda\\n    * 1 teaspoon baking powder\\n    * 1/2 teaspoon salt\\n    * 1 cup buttermilk\\n    * 1/2 cup vegetable oil\\n    * 2 large eggs\\n    * 1 teaspoon vanilla extract\\n    * 1 cup boiling water\\n* **For the Frosting:**\\n    * 1 cup (2 sticks) unsalted butter, softened\\n    * 3 cups powdered sugar\\n    * 1/2 cup unsweetened cocoa powder\\n    * 1/4 cup milk\\n    * 1 teaspoon vanilla extract\\n\\n**Instructions:**\\n\\n1. **Preheat oven to 350°F (175°C).** Grease and flour two 9-inch round cake pans.\\n2. **Combine dry ingredients:** In a large bowl, whisk together flour, sugar, cocoa powder, baking soda, baking powder, and salt.\\n3. **Combine wet ingredients:** In a separate bowl, whisk together buttermilk, oil, eggs, and vanilla extract.\\n4. **Add wet to dry:** Gradually add the wet ingredients to the dry ingredients, mixing until just combined. Do not overmix.\\n5. **Add boiling water:** Slowly pour in the boiling water, stirring until fully incorporated. The batter will become thin.\\n6. **Pour batter into pans:** Divide the batter evenly between the prepared cake pans.\\n7. **Bake:** Bake for 30-35 minutes, or until a toothpick inserted into the center comes out clean.\\n8. **Cool cakes:** Let the cakes cool in the pans for 10 minutes before inverting them onto a wire rack to cool completely.\\n9. **Make frosting:** In a large bowl, cream together butter and powdered sugar until light and fluffy. Add cocoa powder, milk, and vanilla extract, beating until smooth.\\n10. **Frost the cake:** Once the cakes are completely cool, frost the bottom layer with half of the frosting. Top with the second cake layer and frost the entire cake with the remaining frosting.\\n11. **Decorate:** Decorate the cake as desired.\\n\\n**Tips:**\\n\\n* For a richer flavor, use Dutch-processed cocoa powder.\\n* If you don't have buttermilk, you can make your own by adding 1 tablespoon of lemon juice or white vinegar to 1 cup of milk.\\n* For a fudgier cake, use less flour and more cocoa powder.\\n* To prevent the cake from sticking to the pans, line them with parchment paper.\\n* Let the cake cool completely before frosting to prevent the frosting from melting.\\n* Store the cake in an airtight container at room temperature for up to 3 days.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ErYtGHm5f6wq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}